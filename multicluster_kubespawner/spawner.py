import asyncio
from tornado import gen
from io import StringIO
from jinja2 import Template
from textwrap import dedent
import tempfile
import string
import escapism
from ruamel.yaml import YAML
import json

yaml = YAML(typ="safe")

from jupyterhub.spawner import Spawner
from traitlets.config import Unicode, Dict, List
from traitlets import default, Union, Callable, Integer, Bool


def make_dns_safe(s: str) -> str:
    """
    escape a string to be DNS safe
    """
    # Make sure username and servername match the restrictions for DNS labels
    # Note: '-' is not in safe_chars, as it is being used as escape character
    safe_chars = set(string.ascii_lowercase + string.digits)

    return escapism.escape(s, safe=safe_chars, escape_char="-").lower()


class MultiClusterKubeSpawner(Spawner):

    port = Integer(
        8888,
        help="""
        The port the user server process listens on inside the container.

        Since each pod gets its own network stack with its own IP, all servers
        can listen on the same well known port 8888. A JupyterHub operator will
        most likely not need to change this.
        """,
        config=True,
    )

    # Explicitly setting properties here that are traitlets takes them out of
    # the autogenerated traitlets documentation, which is great. We don't want
    # some of these options to actually be configurable!

    # Notebook servers must always listen on 0.0.0.0, otherwise there is no
    # way for the Ingress providers to talk to them
    ip = "0.0.0.0"

    # Disabling user config doesn't really 'work' with containers, so let's not
    # allow people to do that here.
    disable_user_config = False

    # We want no env vars from JupyterHub process to get into the singleuser servers,
    # as they run in totally different environments.
    env_keep = []

    # Namespace to create when we are creating a namespace per user
    namespace_resource_template = Template(
        """
        apiVersion: v1
        kind: Namespace
        metadata:
            labels:
                kubernetes.io/metadata.name: {{spawner.namespace}}
                mcks.hub.jupyter.org/delete-on-stop: "false"
                name: {{spawner.namespace}}
            name: {{spawner.namespace}}
        """
    )

    # Default set of kubernetes resources created for each user
    default_resources = {
        # sa must come before pod, as pod references sa
        "01-sa": """
            apiVersion: v1
            kind: ServiceAccount
            metadata:
                name: {{key}}
        """,
        "02-pod": """
            apiVersion: v1
            kind: Pod
            metadata:
                name: {{key}}
            spec:
                serviceAccountName: {{key}}
                containers:
                - name: notebook
                  image: {{spawner.image}}
                  command: {{spawner.cmd|tojson}}
                  args: {{spawner.get_args()|tojson}}
                  resources: {{resources|tojson}}
                  ports:
                  - containerPort: {{spawner.port}}
                  env:
                  # The memory env vars can be set directly by kubernetes, as they just show up
                  # as 'bytes'. The CPU ones are a bit more complicated, because kubernetes will
                  # only provide integers, with a single unit being 1m or .001 of a CPU. JupyterHub
                  # says they'll be floats, as fractions of a full CPU. There isn't really a way to
                  # do that in kubernetes, so we've to resort to doing that manually. This kinda sucks.
                  # An advantage with kubernetes would be that it knows the *real* limits, which can be
                  # setup either by a LimitRange resource, or by just the number of CPUs available in the
                  # node. Our spawner doesn't have this information.
                  - name: MEM_GUARANTEE
                    valueFrom:
                        resourceFieldRef:
                            containerName: notebook
                            resource: requests.memory
                  - name: MEM_LIMIT
                    valueFrom:
                        resourceFieldRef:
                            containerName: notebook
                            resource: limits.memory
                  {% for k, v in spawner.get_env().items() -%}
                  - name: {{k}}
                    value: {{v|tojson}}
                  {% endfor %}
        """,
        "03-service": """
            apiVersion: v1
            kind: Service
            metadata:
                name: {{key}}
            spec:
                selector:
                    mcks.hub.jupyter.org/key: {{key}}
                ports:
                    - protocol: TCP
                      port: 8888
                      targetPort: 8888
            """,
        "04-ingress": """
            apiVersion: networking.k8s.io/v1
            kind: Ingress
            metadata:
                name: {{key}}
                annotations:
                    # Required to get websockets to work with contour
                    projectcontour.io/websocket-routes: {{proxy_spec}}
                    contour.heptio.com/websocket-routes: {{proxy_spec}}
            spec:
                ingressClassName: contour
                rules:
                - http:
                    paths:
                    - path: {{proxy_spec}}
                      pathType: Prefix
                      backend:
                        service:
                          name: {{key}}
                          port:
                            number: 8888
        """,
    }

    resources = Dict(
        Unicode,
        default={},
        help="""
        Jinja2 Template to generate kubernetes resources generated by the spawner.

        Resources are sorted by key before they are evaluated.
        """,
        config=True,
    )

    key_template = Unicode("jupyter-{{username}}--{{servername}}", config=True)
    namespace_template = Unicode(
        "jupyter-{{username}}",
        help="""
        Jinja2 template to generate namespace each user is spawned into.
        """,
        config=True,
    )

    create_namespace = Bool(
        True,
        help="""
        Create namespace for each user when needed.

        For each user, a namespace is created if needed based on namespace_template.

        If set to false, set namespace_template to always resolve to a string that
        points to an existing template.
        """,
        config=True,
    )

    patches = Dict(
        Unicode,
        {},
        help="""
        Customize generated resources by patching them where necessary.

        A jinja2 template that can produce multiple YAML documents, which will be
        merged with generated resources from resources. metadata.name will
        be used to decide which resources are patched.

        The patches are sorted by key before they are evaluated.
        """,
        config=True,
    )

    ingress_public_url = Unicode(
        "",
        help="""
        Address of the ingress controller's public endpoint in the targe cluster
        """,
        config=True,
    )

    kubernetes_context = Unicode(
        "",
        help="""
        Kubernetes context to use for connecting to the kubernetes cluster.
        """,
        config=True,
    )

    image = Unicode(
        "pangeo/pangeo-notebook:latest",
        config=True,
        help="""
        Docker image to use for spawning user's containers.

        Defaults to `pangeo/pangeo-notebook:latest`

        Name of the container + a tag, same as would be used with
        a `docker pull` command. If tag is set to `latest`, kubernetes will
        check the registry each time a new user is spawned to see if there
        is a newer image available. If available, new image will be pulled.
        Note that this could cause long delays when spawning, especially
        if the image is large. If you do not specify a tag, whatever version
        of the image is first pulled on the node will be used, thus possibly
        leading to inconsistent images on different nodes. For all these
        reasons, it is recommended to specify a specific immutable tag
        for the image.

        If your image is very large, you might need to increase the timeout
        for starting the single user container from the default. You can
        set this with::

           c.Spawner.start_timeout = 60 * 5  # Up to 5 minutes

        """,
    )

    profile_list = Union(
        trait_types=[List(trait=Dict()), Callable()],
        config=True,
        help="""
        List of profiles to offer for selection by the user.

        Signature is: `List(Dict())`, where each item is a dictionary that has two keys:

        - `display_name`: the human readable display name (should be HTML safe)
        - `slug`: the machine readable slug to identify the profile
          (missing slugs are generated from display_name)
        - `description`: Optional description of this profile displayed to the user.
        - `spawner_override`: a dictionary with overrides to apply to the Spawner
          settings. Each value can be either the final value to change or a callable that
          take the `Spawner` instance as parameter and return the final value.
        - `default`: (optional Bool) True if this is the default selected option

        Instead of a list of dictionaries, this could also be a callable that takes as one
        parameter the current spawner instance and returns a list of dictionaries. The
        callable will be called asynchronously if it returns a future, rather than
        a list. Note that the interface of the spawner class is not deemed stable
        across versions, so using this functionality might cause your JupyterHub
        or MultiClusterKubeSpawner upgrades to break.
        """,
    )
    profile_form_template = Unicode(
        """
        <style>
        /* The profile description should not be bold, even though it is inside the <label> tag */
        #multicluster-kubespawner-profiles-list label p {
            font-weight: normal;
        }
        </style>

        <div class='form-group' id='multicluster-kubespawner-profiles-list'>
        {% for profile in profile_list %}
        <label for='profile-item-{{ profile.slug }}' class='form-control input-group'>
            <div class='col-md-1'>
                <input type='radio' name='profile' id='profile-item-{{ profile.slug }}' value='{{ profile.slug }}' {% if profile.default %}checked{% endif %} />
            </div>
            <div class='col-md-11'>
                <strong>{{ profile.display_name }}</strong>
                {% if profile.description %}
                <p>{{ profile.description }}</p>
                {% endif %}
            </div>
        </label>
        {% endfor %}
        </div>
        """,
        config=True,
        help="""
        Jinja2 template for constructing profile list shown to user.

        Used when `profile_list` is set.

        The contents of `profile_list` are passed in to the template.
        This should be used to construct the contents of a HTML form. When
        posted, this form is expected to have an item with name `profile` and
        the value the index of the profile in `profile_list`.
        """,
    )

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Key depends on other params here, so do it last
        self.key = Template(self.key_template).render(**self.template_vars).rstrip("-")
        self.namespace = Template(self.namespace_template).render(**self.template_vars)

        # Store a list of resources we created so we can clean them up
        self.created_resources = []

    @property
    def template_vars(self) -> dict:
        raw_servername = self.name or ""
        safe_servername = make_dns_safe(raw_servername)

        safe_username = make_dns_safe(self.user.name)
        params = dict(
            userid=self.user.id,
            username=safe_username,
            unescaped_username=self.user.name,
            servername=safe_servername,
            unescaped_servername=raw_servername,
            proxy_spec=self.proxy_spec,
            spawner=self,
        )

        return params

    def get_labels(self) -> dict:
        """
        Default labels added on to all resources generated by this spawner
        """
        return {
            "mcks.hub.jupyter.org/key": self.key,
        }

    async def apply_patches(self, resources: List) -> List:
        params = self.template_vars.copy()
        params["key"] = self.key

        named_resources = {f"{o['kind']}/{o['metadata']['name']}": o for o in resources}

        patches = [
            yaml.load(Template(p).render(**params))
            for k, p in sorted(self.patches.items())
        ]

        for patch in patches:
            patch_name = f"{patch['kind']}/{patch['metadata']['name']}"
            with tempfile.NamedTemporaryFile(mode="w") as f:
                yaml.dump(named_resources[patch_name], f)
                f.flush()
                cmd = [
                    "kubectl",
                    "patch",
                    "-n",
                    self.namespace,
                    "--local",
                    "-f",
                    f.name,
                    "--patch",
                    json.dumps(patch),
                    "-o",
                    "yaml",
                ]
                if self.kubernetes_context:
                    cmd.append(f"--context={self.kubernetes_context}")

                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )

                stdout, stderr = await proc.communicate()
                if await proc.wait() != 0:
                    raise ValueError(
                        f"kubectl patch failed for {patch_name}: {stdout}, {stderr}"
                    )
                named_resources[patch_name] = yaml.load(stdout.decode())

        return list(named_resources.values())

    def get_env(self) -> dict:
        # jupyterhub sets {CPU|MEM}_{LIMIT|GUARANTEE} env vars if appropriate
        # cpu and mem traitlets are set in the Spawner. We instead set these in
        # augment_notebook_container based on what the state of the pod is,
        # so patches can work properly as well.
        env = super().get_env()
        if "CPU_LIMIT" in env:
            del env["CPU_LIMIT"]
        if "CPU_GUARANTEE" in env:
            del env["CPU_GUARANTEE"]
        if "MEM_LIMIT" in env:
            del env["MEM_LIMIT"]
        if "MEM_GUARANTEE" in env:
            del env["MEM_GUARANTEE"]

        return env

    def augment_notebook_container(self, resources: list) -> list:
        """
        Augment the notebook container resource after all patches are done

        Used to set JUPYTER_IMAGE and JUPYTER_IMAGE_SPEC env vars, as they can
        be modified by patches, but we can't refer to them from the env directly.
        """
        for resource in resources:
            if resource["kind"] == "Pod" and resource["metadata"]["name"] == self.key:
                for c in resource["spec"]["containers"]:
                    if c["name"] == "notebook":
                        c["env"].append({"name": "JUPYTER_IMAGE", "value": c["image"]})
                        c["env"].append(
                            {"name": "JUPYTER_IMAGE_SPEC", "value": c["image"]}
                        )

                        container_resources = c.setdefault("resources", {})

                        # The memory env vars can be set directly by kubernetes, as they just show up
                        # as 'bytes'. The CPU ones are a bit more complicated, because kubernetes will
                        # only provide integers, with a single unit being 1m or .001 of a CPU. JupyterHub
                        # says they'll be floats, as fractions of a full CPU. There isn't really a way to
                        # do that in kubernetes, so we've to resort to doing that manually. This kinda sucks.
                        # An advantage with kubernetes would be that it knows the *real* limits, which can be
                        # setup either by a LimitRange resource, or by just the number of CPUs available in the
                        # node. Our spawner doesn't have this information.
                        if (
                            "limits" in container_resources
                            and "cpu" in container_resources["limits"]
                        ):
                            cpu_limit_str = str(container_resources["limits"]["cpu"])
                            if cpu_limit_str.endswith("m"):
                                cpu_limit_str = int(cpu_limit_str.rstrip("m")) / 1000
                            c["env"].append(
                                {
                                    "name": "CPU_LIMIT",
                                    "value": f"{float(cpu_limit_str):0.2f}",
                                }
                            )
                        if (
                            "requests" in container_resources
                            and "cpu" in container_resources["requests"]
                        ):
                            cpu_guarantee_str = str(
                                container_resources["requests"]["cpu"]
                            )
                            if cpu_guarantee_str.endswith("m"):
                                cpu_guarantee_str = (
                                    int(cpu_guarantee_str.rstrip("m")) / 1000
                                )
                            c["env"].append(
                                {
                                    "name": "CPU_GUARANTEE",
                                    "value": f"{float(cpu_guarantee_str):0.2f}",
                                }
                            )

                        return resources

    def get_resources_spec(self) -> list:
        """
        Render the templated YAML
        """
        params = self.template_vars.copy()
        params["key"] = self.key

        resources = {"limits": {}, "requests": {}}
        if self.mem_guarantee:
            resources["requests"]["memory"] = str(self.mem_guarantee)
        if self.mem_limit:
            resources["limits"]["memory"] = str(self.mem_limit)
        if self.cpu_guarantee:
            resources["requests"]["cpu"] = str(self.cpu_guarantee)
        if self.cpu_limit:
            resources["limits"]["cpu"] = str(self.cpu_limit)

        params["resources"] = resources

        all_resources = self.default_resources.copy()
        all_resources.update(self.resources)

        rendered = [
            Template(dedent(o)).render(**params)
            for k, o in sorted(all_resources.items())
        ]
        parsed = []
        for r in rendered:
            # FIXME: report YAML parse errors clearly
            parsed.append(yaml.load(r))

        for p in parsed:
            # Inject metadata into every resource
            labels = p.setdefault("metadata", {}).setdefault("labels", {})
            labels.update(self.get_labels())

        return parsed

    def get_state(self) -> dict:
        """
        Save state required to reinstate this user's pod from scratch
        """
        state = super().get_state()
        state["key"] = self.key
        state["kubernetes_context"] = self.kubernetes_context
        state["ingress_public_url "] = self.ingress_public_url
        state["created_resources"] = self.created_resources
        return state

    def load_state(self, state: dict):
        """
        Load state from storage required to reinstate this user's pod
        """
        if "key" in state:
            self.key = state["key"]
        if "ingress_public_url" in state:
            self.ingress_public_url = state["ingress_public_url"]
        if "kubernetes_context" in state:
            self.kubernetes_context = state["kubernetes_context"]
        if "created_resources" in state:
            self.created_resources = state["created_resources"]

    async def kubectl_apply(self, spec: list):
        cmd = [
            "kubectl",
            "apply",
            "-n",
            self.namespace,
            "--wait",  # Wait for resources to be 'ready' before returning
            "-f",
            "-",
            "-o",
            "yaml",
        ]
        if self.kubernetes_context:
            cmd.append(f"--context={self.kubernetes_context}")
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        with StringIO() as s:
            yaml.dump_all(spec, s)
            s.seek(0)
            objs = s.read()
        self.log.debug(f"kubectl applying {objs}")
        stdout, stderr = await proc.communicate(objs.encode())

        if (await proc.wait()) != 0:
            raise ValueError(f"kubectl apply failed: {stdout}, {stderr}")

    async def kubectl_wait(self, timeout=30):
        cmd = [
            "kubectl",
            "-n",
            self.namespace,
            "wait",
            "--for=condition=Ready",
            f"pod/{self.key}",
            f"--timeout={timeout}s",
        ]
        if self.kubernetes_context:
            cmd.append(f"--context={self.kubernetes_context}")
        proc = await asyncio.create_subprocess_exec(*cmd)
        stdout, stderr = await proc.communicate()
        return await proc.wait()

    async def start(self):
        # load user options (including profile)
        await self.load_user_options()

        # Generate YAML spec to be applied by rendering our resource templates (self.resources),
        # applying any patches defined in self.patches, and finally augmenting the notebook
        # container specifically with things that will be too cumborsome to do in jinja2 or
        # depend on properties that could be changed by any of the patches
        self.created_resources = self.augment_notebook_container(
            await self.apply_patches(self.get_resources_spec())
        )

        if self.create_namespace:
            params = self.template_vars.copy()
            params["key"] = self.key
            template_resource = yaml.load(
                self.namespace_resource_template.render(**params)
            )
            self.created_resources.insert(0, template_resource)

        created_resource_names = " ".join(
            f"{r['kind']}/{r['metadata']['name']}" for r in self.created_resources
        )
        self.log.info(
            f"Creating resources for user {self.user.name}: {created_resource_names} in namespace {self.namespace}"
        )
        await self.kubectl_apply(self.created_resources)
        await self.kubectl_wait(self.start_timeout)

        # We aren't waiting long enough for the ingress resource to be fully
        # picked up by the controllers.
        # FIXME: Wait for ingress to become ready instead
        await asyncio.sleep(1)

        # We always just return the public URL of the ingress provider, as both
        # our proxy and the ingress controller on the target cluster keep the
        # url path intact, and route to the correct pod
        return self.ingress_public_url

    async def stop(self):
        # delete all doesn't seem to delete ingresses, lol?!
        # https://github.com/kubernetes/kubectl/issues/7
        cmd = ["kubectl", "delete", "-n", self.namespace, "-f", "-", "--wait"]
        if self.kubernetes_context:
            cmd.append(f"--context={self.kubernetes_context}")
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        # Delete everything that doesn't have a special label telling us to not do that
        resources_to_delete = [
            r
            for r in self.created_resources
            if r.get("metadata", {})
            .get("labels", {})
            .get("mcks.hub.jupyter.org/delete-on-stop", "true")
            .lower()
            == "true"
        ]
        deleted_resource_names = " ".join(
            f"{r['kind']}/{r['metadata']['name']}" for r in resources_to_delete
        )
        self.log.info(
            f"Deleting resources for user {self.user.name}: {deleted_resource_names} in namespace {self.namespace}"
        )
        with StringIO() as s:
            yaml.dump_all(resources_to_delete, s)
            s.seek(0)
            resources_yaml = s.read()
        stdout, stderr = await proc.communicate(resources_yaml.encode())

        ret = await proc.wait()

        if ret != 0:
            raise ValueError(
                f"kubectl delete failed with code {ret}: {stdout}, {stderr}"
            )

    async def poll(self):
        ret = await self.kubectl_wait()
        if ret == 0:
            return None
        return ret

    _profile_list = None

    def _render_options_form(self, profile_list: list):
        self._profile_list = self._init_profile_list(profile_list)
        profile_form_template = Template(self.profile_form_template)
        return profile_form_template.render(profile_list=self._profile_list)

    async def _render_options_form_dynamically(self, current_spawner):
        profile_list = await gen.maybe_future(self.profile_list(current_spawner))
        profile_list = self._init_profile_list(profile_list)
        return self._render_options_form(profile_list)

    @default("options_form")
    def _options_form_default(self):
        """
        Build the form template according to the `profile_list` setting.

        Returns:
            '' when no `profile_list` has been defined
            The rendered template (using jinja2) when `profile_list` is defined.
        """
        if not self.profile_list:
            return ""
        if callable(self.profile_list):
            return self._render_options_form_dynamically
        else:
            return self._render_options_form(self.profile_list)

    @default("options_from_form")
    def _options_from_form_default(self):
        return self._options_from_form

    def _options_from_form(self, formdata):
        """get the option selected by the user on the form

        This only constructs the user_options dict,
        it should not actually load any options.
        That is done later in `.load_user_options()`

        Args:
            formdata: user selection returned by the form

        To access to the value, you can use the `get` accessor and the name of the html element,
        for example::

            formdata.get('profile',[0])

        to get the value of the form named "profile", as defined in `form_template`::

            <select class="form-control" name="profile"...>
            </select>

        Returns:
            user_options (dict): the selected profile in the user_options form,
                e.g. ``{"profile": "cpus-8"}``
        """
        return {"profile": formdata.get("profile", [None])[0]}

    async def _load_profile(self, slug: str):
        """Load a profile by name

        Called by load_user_options
        """

        # find the profile
        default_profile = self._profile_list[0]
        for profile in self._profile_list:
            if profile.get("default", False):
                # explicit default, not the first
                default_profile = profile

            if profile["slug"] == slug:
                break
        else:
            if slug:
                # name specified, but not found
                raise ValueError(
                    "No such profile: %s. Options include: %s"
                    % (slug, ", ".join(p["slug"] for p in self._profile_list))
                )
            else:
                # no name specified, use the default
                profile = default_profile

        self.log.debug(
            "Applying Spawner override for profile '%s'", profile["display_name"]
        )
        spawner_override = profile.get("spawner_override", {})
        for k, v in spawner_override.items():
            if callable(v):
                v = v(self)
                self.log.debug(
                    ".. overriding Spawner value %s=%s (callable result)", k, v
                )
            else:
                self.log.debug(".. overriding Spawner value %s=%s", k, v)
            setattr(self, k, v)

    # set of recognised user option keys
    # used for warning about ignoring unrecognised options
    _user_option_keys = {
        "profile",
    }

    def _init_profile_list(self, profile_list: list) -> list:
        # generate missing slug fields from display_name
        for profile in profile_list:
            if "slug" not in profile:
                profile["slug"] = make_dns_safe(profile["display_name"])

        return profile_list

    async def load_user_options(self):
        """Load user options from self.user_options dict

        This can be set via POST to the API or via options_from_form

        Only supported argument by default is 'profile'.
        Override in subclasses to support other options.
        """

        if self._profile_list is None:
            if callable(self.profile_list):
                profile_list = await gen.maybe_future(self.profile_list(self))
            else:
                profile_list = self.profile_list

            self._profile_list = self._init_profile_list(profile_list)

        selected_profile = self.user_options.get("profile", None)
        if self._profile_list:
            await self._load_profile(selected_profile)
        elif selected_profile:
            self.log.warning(
                "Profile %r requested, but profiles are not enabled", selected_profile
            )

        # help debugging by logging any option fields that are not recognized
        option_keys = set(self.user_options)
        unrecognized_keys = option_keys.difference(self._user_option_keys)
        if unrecognized_keys:
            self.log.warning(
                "Ignoring unrecognized Spawner user_options: %s",
                ", ".join(map(str, sorted(unrecognized_keys))),
            )
